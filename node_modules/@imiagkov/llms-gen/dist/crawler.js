import axios from "axios";
import PQueue from "p-queue";
import * as cheerio from "cheerio";
import { normalizeUrl, sleep, log } from "./utils.js";
import { fetchRobots } from "./robots.js";
import { XMLParser } from "fast-xml-parser";
import { chromium } from "playwright";
export function applyFilters(url, include, exclude) {
    if (exclude && exclude.some((re) => re.test(url)))
        return false;
    if (include && include.length > 0)
        return include.some((re) => re.test(url));
    return true;
}
export async function discoverFromSitemaps(sitemaps, opts) {
    const found = new Set();
    const parser = new XMLParser({ ignoreAttributes: false });
    const logCrawler = opts.logCrawler;
    for (const sm of sitemaps) {
        if (found.size >= opts.maxPages)
            break;
        try {
            if (logCrawler) {
                log(`[crawler] Fetching sitemap: ${sm}`);
            }
            const res = await axios.get(sm, {
                timeout: 10000,
                headers: { "User-Agent": opts.userAgent, Accept: "application/xml, text/xml, */*", "Accept-Encoding": "gzip, deflate, br" }
            });
            const xml = typeof res.data === "string" ? res.data : res.data?.toString?.() ?? "";
            const j = parser.parse(xml);
            if (j.sitemapindex?.sitemap) {
                const list = Array.isArray(j.sitemapindex.sitemap) ? j.sitemapindex.sitemap : [j.sitemapindex.sitemap];
                for (const s of list) {
                    if (s.loc) {
                        const nested = await discoverFromSitemaps([s.loc], opts);
                        nested.forEach((u) => found.add(u));
                        if (found.size >= opts.maxPages)
                            break;
                    }
                }
            }
            else if (j.urlset?.url) {
                const list = Array.isArray(j.urlset.url) ? j.urlset.url : [j.urlset.url];
                if (logCrawler) {
                    log(`[crawler] Found sitemap with ${list.length} URLs`);
                }
                for (const u of list) {
                    if (!u.loc)
                        continue;
                    found.add(u.loc); // keep raw, normalize later
                    if (found.size >= opts.maxPages)
                        break;
                }
            }
        }
        catch (e) {
            if (logCrawler) {
                const msg = e instanceof Error ? e.message : String(e);
                log(`[crawler] Warning fetching sitemap ${sm}: ${msg}`);
            }
        }
    }
    return Array.from(found);
}
export async function discoverUrls(opts) {
    const robots = await fetchRobots(opts.rootUrl, opts.userAgent);
    if (opts.logRobots) {
        log("[crawler] Robots.txt rules (partial check):");
        [opts.rootUrl, ...robots.sitemaps].forEach((url) => {
            const allowed = robots.isAllowed(url, opts.userAgent);
            log(`  ${allowed ? "✅" : "❌"} ${url}`);
        });
    }
    const defaultSitemap = new URL("/sitemap.xml", opts.rootUrl).toString();
    const sitemaps = opts.ignoreRobotsForSitemap || robots.sitemaps.length === 0 ? [defaultSitemap] : robots.sitemaps;
    // 1. Try sitemap(s)
    let sitemapUrls = await discoverFromSitemaps(sitemaps, opts);
    log(`Raw sitemap URLs discovered: ${sitemapUrls.length}`);
    // normalize
    sitemapUrls = sitemapUrls
        .map((u) => normalizeUrl(u, opts.rootUrl))
        .filter((u) => !!u);
    log(`After normalize: ${sitemapUrls.length}`);
    // filters
    sitemapUrls = sitemapUrls.filter((u) => applyFilters(u, opts.includePatterns, opts.excludePatterns));
    log(`After include/exclude filters: ${sitemapUrls.length}`);
    if (!opts.ignoreRobotsForSitemap) {
        sitemapUrls = sitemapUrls.filter((u) => robots.isAllowed(u, opts.userAgent));
        log(`After robots.txt filter: ${sitemapUrls.length}`);
    }
    if (sitemapUrls.length > 0) {
        log(`✅ Using ${sitemapUrls.length} URLs from sitemap (skipping BFS)`);
        return sitemapUrls.slice(0, opts.maxPages);
    }
    // 2. Fallback: BFS crawl
    log(`No sitemap found or empty, falling back to BFS crawl from root`);
    const seen = new Set();
    const queue = [opts.rootUrl];
    const pq = new PQueue({ concurrency: opts.concurrency });
    let browser = null;
    let playwrightContexts = null;
    let headlessMode = true; // set false for debugging locally
    if (opts.usePlaywright) {
        try {
            // Launch a single browser instance and reuse it across pages
            browser = await chromium.launch({ headless: headlessMode, args: ["--no-sandbox", "--disable-dev-shm-usage"] });
            // We'll lazily create contexts/pages on demand; keep an array as a simple pool
            playwrightContexts = [];
        }
        catch (err) {
            log(`[crawler] Playwright launch failed: ${err.message || err}`);
            // fallback to axios-based crawling if Playwright can't launch
            browser = null;
            playwrightContexts = null;
        }
    }
    let foundCount = 0;
    while (queue.length && seen.size < opts.maxPages) {
        const current = queue.shift();
        const n = normalizeUrl(current, opts.rootUrl);
        if (!n || seen.has(n))
            continue;
        if (!robots.isAllowed(n, opts.userAgent))
            continue;
        if (!applyFilters(n, opts.includePatterns, opts.excludePatterns))
            continue;
        seen.add(n);
        foundCount++;
        if (opts.logCrawler && foundCount % 10 === 0) {
            log(`[crawler] Extracted metadata for ${foundCount} URLs so far...`);
        }
        await pq.add(async () => {
            try {
                if (opts.delayMs)
                    await sleep(opts.delayMs);
                let html = "";
                if (opts.usePlaywright && browser) {
                    // Acquire a context/page from a small pool (bounded by concurrency)
                    // Try to find an idle context or create one if pool smaller than concurrency
                    let ctxObj = null;
                    // try to reuse an available context (simple lock via a property)
                    for (const c of playwrightContexts) {
                        if (!c.busy) {
                            c.busy = true;
                            ctxObj = c;
                            break;
                        }
                    }
                    // if none available, create a new context if it's safe
                    if (!ctxObj) {
                        const context = await browser.newContext({
                            userAgent: opts.userAgent,
                            viewport: { width: 1280, height: 800 },
                        });
                        const page = await context.newPage();
                        // small stealth measures
                        await page.addInitScript(() => {
                            Object.defineProperty(navigator, "webdriver", { get: () => false });
                        });
                        ctxObj = { context, page };
                        // push to pool and mark busy
                        playwrightContexts.push({ ...ctxObj, busy: true });
                    }
                    const page = ctxObj.page;
                    const timeoutMs = opts.scrapeTimeoutMs ?? 30000;
                    try {
                        // Use domcontentloaded instead of networkidle (less likely to hang)
                        await page.goto(n, { waitUntil: "domcontentloaded", timeout: timeoutMs });
                        // small extra wait for JS rendering (tweak as needed)
                        await page.waitForTimeout(1000);
                        html = await page.content();
                    }
                    catch (err) {
                        const msg = err instanceof Error ? err.message : String(err);
                        log(`[crawler] Warning fetching ${n}: ${msg}`);
                    }
                    finally {
                        // release context back to pool (keep the page/context alive for reuse)
                        for (const c of playwrightContexts) {
                            if (c.page === page) {
                                c.busy = false;
                                break;
                            }
                        }
                    }
                }
                else {
                    // axios fallback
                    const res = await axios.get(n, {
                        timeout: 10000,
                        headers: { "User-Agent": opts.userAgent, Accept: "text/html" },
                    });
                    html = res.data;
                }
                if (html) {
                    const $ = cheerio.load(html);
                    $("a[href]").each((_, el) => {
                        const href = $(el).attr("href");
                        if (!href)
                            return;
                        const nu = normalizeUrl(href, opts.rootUrl);
                        if (!nu)
                            return;
                        if (!robots.isAllowed(nu, opts.userAgent))
                            return;
                        if (!applyFilters(nu, opts.includePatterns, opts.excludePatterns))
                            return;
                        if (!seen.has(nu) && !queue.includes(nu))
                            queue.push(nu);
                    });
                }
            }
            catch (err) {
                // you can log here for debugging
                const msg = err instanceof Error ? err.message : String(err);
                log(`[crawler] Error crawling ${current}: ${msg}`);
            }
        });
    }
    await pq.onIdle();
    if (browser) {
        try {
            // close all contexts/pages first (optional)
            if (playwrightContexts) {
                for (const c of playwrightContexts) {
                    try {
                        await c.context.close();
                    }
                    catch { }
                }
            }
            await browser.close();
        }
        catch (err) {
            // ignore close errors
        }
    }
    const combined = Array.from(seen);
    log(`Discovered ${combined.length} URLs via BFS crawl`);
    return combined.slice(0, opts.maxPages);
}
