import { discoverUrls } from "./crawler.js";
import { fetchMeta } from "./extractor.js";
import { writeFiles } from "./generator.js";
import { startMonitor } from "./monitor.js";
import { hasWebsiteChanged, getSitemapUrl, log } from "./utils.js";
const DEFAULT_UA = "llms-gen-bot/1.0 (+https://npmjs.com/)";
export async function crawlSite(options) {
    const { rootUrl, maxPages = 500, userAgent = DEFAULT_UA, concurrency = 8, delayMs, includePatterns, excludePatterns, usePlaywright, scrapeTimeoutMs, ignoreRobotsForSitemap, logRobots, logCrawler, } = options;
    return discoverUrls({
        rootUrl,
        maxPages,
        userAgent,
        concurrency,
        delayMs,
        includePatterns,
        excludePatterns,
        usePlaywright,
        scrapeTimeoutMs,
        ignoreRobotsForSitemap,
        logRobots,
        logCrawler
    });
}
/**
 * Fetch metadata for a list of URLs with bounded concurrency.
 * - urls: array of URLs to fetch
 * - userAgent: UA string passed to fetchMeta
 * - concurrency: number of concurrent fetchers (default 8)
 */
export async function extractMetadata(urls, userAgent = DEFAULT_UA, concurrency = 8) {
    if (!urls || urls.length === 0)
        return [];
    const total = urls.length;
    const out = new Array(total);
    let nextIndex = 0;
    let processed = 0;
    const progressInterval = Math.max(1, Math.floor(total / 10));
    // worker loop â€” each worker takes next index until exhausted
    async function worker() {
        while (true) {
            const idx = nextIndex++;
            if (idx >= urls.length)
                return;
            const u = urls[idx];
            try {
                const meta = await fetchMeta(u, userAgent);
                out[idx] = meta;
            }
            catch (err) {
                // graceful fallback on per-URL error
                out[idx] = { url: u, score: 0 };
            }
            finally {
                processed++;
                // Log progress every progressInterval items (roughly 10% increments)
                if (processed % progressInterval === 0 || processed === total) {
                    log(`[extractor] Extracted metadata for ${processed} / ${total} URLs`);
                }
            }
        }
    }
    // spawn bounded number of workers
    const workers = Array.from({ length: Math.max(1, Math.min(concurrency, urls.length)) }, () => worker());
    await Promise.all(workers);
    // Remove any undefined slots (shouldn't happen but safe)
    const filled = out.map((v, idx) => v ?? { url: urls[idx], score: 0 });
    // rank by score, then title length
    filled.sort((a, b) => (b.score || 0) - (a.score || 0) || (b.title?.length || 0) - (a.title?.length || 0));
    return filled;
}
/**
 * Generate llms.txt (only) for the given options.
 * returns { shortTxt, shortPath }
 */
export async function generateLlmsTxt(opts) {
    const { rootUrl, maxPages = 500, outputDir = process.cwd(), userAgent = DEFAULT_UA, concurrency = 8, delayMs, includePatterns, excludePatterns, usePlaywright, scrapeTimeoutMs, ignoreRobotsForSitemap, logRobots, logCrawler } = opts;
    const urls = await crawlSite({
        rootUrl,
        maxPages,
        userAgent,
        concurrency,
        delayMs,
        includePatterns,
        excludePatterns,
        usePlaywright,
        scrapeTimeoutMs,
        ignoreRobotsForSitemap,
        logRobots,
        logCrawler
    });
    const metas = await extractMetadata(urls, userAgent);
    // writeFiles now only produces llms.txt (short)
    return writeFiles(rootUrl, metas, {
        outputDir,
    });
}
/**
 * Generate and write only if the file content changed.
 * Returns the same object as generateLlmsTxt when changed, otherwise null.
 */
export async function generateIfChanged(opts) {
    const { rootUrl, maxPages = 500, outputDir = process.cwd(), userAgent = DEFAULT_UA, concurrency = 8, delayMs, includePatterns, excludePatterns, usePlaywright, scrapeTimeoutMs, ignoreRobotsForSitemap, logRobots, logCrawler } = opts;
    const target = getSitemapUrl(rootUrl);
    const changed = await hasWebsiteChanged([target], outputDir, userAgent);
    if (!changed)
        return null;
    const res = await generateLlmsTxt(opts);
    return res;
}
/**
 * Start monitoring the given site and call generateIfChanged when the monitor detects a change.
 * Returns a controller { stop() } which you can call to stop monitoring.
 *
 * monitorOpts fields:
 *  - rootUrl: string (required)
 *  - intervalMs: number (ms between checks)
 *  - userAgent?: string
 *  - preferSitemap?: boolean
 *  - onChange?: (res) => void
 *  - onError?: (err) => void
 */
export function startMonitoring(monitorOpts) {
    const { rootUrl, intervalMs, userAgent, preferSitemap, onChange, onError, ...generateOpts } = monitorOpts;
    if (!rootUrl)
        throw new Error("startMonitoring requires rootUrl");
    // the trigger calls generateIfChanged with the generate options
    const trigger = async () => {
        try {
            const res = await generateIfChanged({
                rootUrl,
                outputDir: generateOpts.outputDir,
                maxPages: generateOpts.maxPages,
                userAgent: userAgent,
                concurrency: generateOpts.concurrency,
                delayMs: generateOpts.delayMs,
                usePlaywright: generateOpts.usePlaywright,
                scrapeTimeoutMs: generateOpts.scrapeTimeoutMs,
                ignoreRobotsForSitemap: generateOpts.ignoreRobotsForSitemap,
                logRobots: generateOpts.logRobots,
                logCrawler: generateOpts.logCrawler
            });
            if (res) {
                onChange?.(res);
            }
            return res;
        }
        catch (err) {
            onError?.(err);
            throw err;
        }
    };
    return startMonitor({
        rootUrl,
        intervalMs,
        userAgent,
        preferSitemap,
        onChange,
        onError
    }, trigger);
}
export * from "./types.js";
